# Model Tests

This guide explains how to run the pytest test suites in [aiu-fms-testing-utils](https://github.com/aiu-fms-testing-utils/tree/main/tests/models) to validate models and debug issues.

1. [Generate metrics in GPU](MODEL.md#1-run-on-gpu)
2. [Get thresholds](MODEL.md#2-get-thresholds)
3. [Apply thresholds to `test_decoders.py`](MODEL.md#3-apply-thresholds-to-test_decoderspy)
4. [Run `test_model_expectations.py`](MODEL.md#4-run-test_model_expectationspy)

![diagram](./resources/assets/test_flow_diagram.png)

## Test Scripts

### **test_decoders.py**

This test suite evaluates decoder models (i.e., for text generation) across a configurable set of shapes and parameters such as batch_size, prompt_length, max_new_tokens, metrics_thresholds, and failure_rate_thresholds.

#### Single-AIU Run

Example:

```bash
# Set up environment variables for the test run
export FMS_TEST_SHAPES_COMMON_BATCH_SIZES=1
export FMS_TEST_SHAPES_COMMON_SEQ_LENGTHS=128
export FMS_TEST_SHAPES_COMMON_MODEL_PATHS=/ibm-granite/granite-3.3-8b-instruct/
export FMS_TEST_SHAPES_USE_MICRO_MODELS=0

# Run the decoder test
pytest tests/models/test_decoders.py
```

This configuration runs the decoder test on `granite-3.3-8b-instruct` with batch size `1` and sequence length `128`. Set `FMS_TEST_SHAPES_USE_MICRO_MODELS` to test with micro models.

#### Multi-AIU Run

To run the test across multiple AIUs, use `torchrun`:

```bash
torchrun --nproc-per-node=4 -m pytest tests/models/test_decoders.py
```

Environment variables for batch sizes, sequence lengths, and model paths still apply, while `--nproc-per-node=<num_aius>` specifies the number of AIUs to use.

### **test_model_expectations**

This test suite captures and verifies model output on model initialization.

- Add the model path to `models` or `tuple_output_models`.
- To generate expectations, run with the `--capture_expectation` flag. This will create a resource file capturing the model's outputs.
- On subsequent runs, omit the flag to compare live outputs against the saved expectations.

## Metric Thresholds for `test_decoders.py`

To test each model, baseline metrics must first be generated by following the steps outlined below. During the test, the model outputs are compared against these baselines using four key metrics, calculated using the top-k probabilities per token. Their implementations can be found in the [generate_metrics.py](../scripts/generate_metrics.py) script:

### cross_entropy

Cross-entropy measures the divergence between two probability distributions, e.g. the model's predicted token distribution versus the expected output. It quantifies how well the model is predicting the next token. A lower cross-entropy indicates a closer match between expected and generated output.

### prob_mean

The mean of the predicted probabilities across tokens. It measures the model's confidence in its predictions. A low probability mean may indicate the model is uncertain or generating incoherent outputs.

### prob_std

The standard deviation of the predicted probabilities measures how much the model's confidence varies across different tokens.
A high standard deviation indicates fluctuating confidence, while a low value implies more consistent predictions.

### diff_mean

This metric captures the average difference between two sets of predictions to measure the model's performance. A lower diff_mean means the outputs are more consistent.

This metrics will be set at the [fail thresholds](./models/test_decoders.py#L182), so **cross_entropy** and **diff_mean** can be used to compare between the GPU generated text output by the same model on AIU.

### Further Reading

- [CrossEntropyLoss Documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)
- [Probabilistic Language Models](https://courses.grainger.illinois.edu/ece598pv/fa2017/Lecture13_LM_YirenWang.pdf)
- [Model-diff](https://arxiv.org/abs/2412.12177#:~:text=%5B2412.12177%5D%20Model%2Ddiff:,%3E%20cs%20%3E%20arXiv:2412.12177)
- [Estimating the Probabilities of Rare Outputs in Language Models](https://arxiv.org/html/2410.13211v1).

## 1. Run on GPU

First, configure the environment variables:

```bash
export MODEL_PATH=/model-path/
export MAX_NEW_TOKENS=128
export BATCH_SIZES=1
export SEQ_LENS=64
export DEFAULT_TYPES="fp16"
export DS_PATH=/resources/sharegpt/share_gpt.json
```

Then, run the metrics generation script. The ShareGPT dataset will be automatically downloaded if it doesn't exist:

```bash
python generate_metrics.py --architecture=hf_pretrained --model_path=$MODEL_PATH --tokenizer=$MODEL_PATH --unfuse_weights --output_dir=/tmp/aiu-fms-testing-utils/output/ --compile_dynamic --max_new_tokens=$MAX_NEW_TOKENS --min_pad_length=$SEQ_LENS --batch_size=$BATCH_SIZES --default_dtype=$DEFAULT_TYPES --sharegpt_path=$DS_PATH --num_test_tokens_per_sequence=1024
```

This will generate csv files with the results of the metrics calculation. Typically, this is run with batch size 1, 8 and sequency length 64, 2048 (4 runs in total). Then, we can run [get_thresholds.py](./resources/get_thresholds.py) to summarize the results and get the single values for each metric as the following.

This will generate `.csv` files containing the results of the metrics calculations. Typically, you run this script with batch size `1` and `8` and sequence length `64` and `2048` (i.e. 4 runs in total). Then, you can use the [get_thresholds.py](./resources/get_thresholds.py)] script to summarize the results and compute a single threshold value per metric.

At the specified `--output_dir path`, you will find both `.out` and `.csv` files generated:

```bash
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.ce.csv
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.0.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.1.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.2.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.3.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.4.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.5.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.6.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cpu_validation_info.7.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.0.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.1.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.2.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.3.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.4.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.5.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.6.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.cuda_validation_info.7.out
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.diff_mean.csv
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.prob_mean.csv
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3_max-new-tokens-128_batch-size-8_seq-length64_dtype-fp16.prob_std.csv
```

## 2. Get thresholds


Once the metrics have been generated, compute the baseline thresholds using [get_thresholds.py](./resources/get_thresholds.py):

```bash
python3 get_thresholds.py --models /tmp/aiu-fms-testing-utils/models/model-name-version-v1 --metrics diff_mean ce --file_base /tmp/aiu-fms-testing-utils/output
```

After running these scripts in a namespace with 1 GPU, these were the generated thresholds:

```bash
python3 get_thresholds.py --models /tmp/aiu-fms-testing-utils/models/Mistral-7B-Instruct-v0.3 --metrics diff_mean ce --file_base /tmp/aiu-fms-testing-utils/output
found 7 metric files
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3 diff_mean 0.0007839603102183846
found 7 metric files
--tmp--aiu-fms-testing-utils--models--Mistral-7B-Instruct-v0.3 ce 2.8364005851745624
```

These values can now be used towards the model test scripts to validate their performance on AIU.

## 3. Apply thresholds to `test_decoders.py`

These are the variables set at deployment:

| Name        | Value
| ------------- | ----------------
| FMS_TEST_SHAPES_COMMON_MODEL_PATHS        | mistralai/Mistral-7B-Instruct-v0.3
| FMS_TEST_SHAPES_FORCE_VALIDATION_LEVEL_1     | 1
| FMS_TEST_SHAPES_COMMON_BATCH_SIZES           | 1
| FMS_TEST_SHAPES_COMMON_SEQ_LENGTHS      | 64
| FMS_TEST_SHAPES_COMMON_MAX_NEW_TOKENS      | 16
| FMS_TEST_SHAPES_USE_MICRO_MODELS  | 0
| FMS_TEST_SHAPES_METRICS_THRESHOLD | 2.8364005851745624,0.0007839603102183846

- Use `FMS_TEST_SHAPES_METRICS_THRESHOLD` to apply custom metric thresholds without modifying any test code.
- To speed up the tests for large models, set `FMS_TEST_SHAPES_VALIDATION_INFO_DIR` to the directory containing output logits saved from generating the metrics.
- Set `FMS_TEST_SHAPES_FAILURE_THRESHOLD` if you would like to relax the threshold (default is `0.01`).

Add the newly generated numbers to the end of the `fail_thresholds` dictionary in [`test_decoders.py`](./models/test_decoders.py), e.g.:

```python
fail_thresholds = {
    (LLAMA_3p1_8B_INSTRUCT, True): (
        3.7392955756187423,
        .001, # FIXME: compute
    ),
    (GRANITE_3p2_8B_INSTRUCT, True): (
        2.996668996810913,
        .001, # FIXME: compute
    ),
    (GRANITE_20B_CODE_INSTRUCT_8K, True): (
        3.7392955756187423, # FIXME: compute -- setting to micro llama 3.1 8b instruct
        .001, # FIXME: compute
    ),
    (LLAMA_3p1_70B_INSTRUCT, True): (
        3.8235735702514626,
        .001, # FIXME: compute
    ),
    (LLAMA_3p1_8B_INSTRUCT, False): (
        2.6994638133048965,
        0.00047589250549208347,
    ),
    (GRANITE_3p2_8B_INSTRUCT, False): (
        2.3919514417648315,
        0.0005767398688476533,
    ),
    (GRANITE_20B_CODE_INSTRUCT_8K, False): (
        2.640706129074097,
        0.00034344267623964697,
    ),
    (LLAMA_3p1_70B_INSTRUCT, False): (
        2.841279556751251,
        0.0044301633024588115,
    ),
}
```

Run the script using the following command, adding the `-vv` flag for verbose output:

```bash
pytest tests/models/test_decoders.py -vv
```

### Test Results Sample

The following is a sample of the test outputs:

```bash
Starting to run pytest tests/models/test_decoders.py
[ 0/ 1]: Sentient AIU: Enabled
============================= test session starts ==============================
platform linux -- Python 3.11.9, pytest-8.3.5, pluggy-1.5.0
rootdir: /tmp/aiu-fms-testing-utils
plugins: durations-1.4.0, env-1.1.5
collected 1 item

tests/models/test_decoders.py .                                          [100%]

=============================== warnings summary ===============================
../foundation-model-stack/fms/triton/pytorch_ops.py:103
  /tmp/foundation-model-stack/fms/triton/pytorch_ops.py:103: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.
    @torch.library.impl_abstract("moe::moe_mm")

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
============================= fixture duration top =============================
total          name               num avg            min           
0:00:00.000140        grand total   5 0:00:00.000014 0:00:00.000012
============================ test call duration top ============================
total          name               num avg            min           
0:02:18.965102 test_common_shapes   1 0:02:18.965102 0:02:18.965102
0:02:18.965102        grand total   1 0:02:18.965102 0:02:18.965102
=========================== test setup duration top ============================
total          name               num avg            min           
0:00:00.000553        grand total   1 0:00:00.000553 0:00:00.000553
========================== test teardown duration top ==========================
total          name               num avg            min           
0:00:00.000969        grand total   1 0:00:00.000969 0:00:00.000969
=================== 1 passed, 1 warning in 140.35s (0:02:20) ===================
Finished running pytests
```

Or, in case of failure:

```bash
[ 0/ 1]: testing model=/mnt/aiu-models-en-shared/models/hf/Mistral-7B-Instruct-v0.3, batch_size=1, seq_length=64, max_new_tokens=16, micro_model=False
[ 0/ 1]: AIU warmup
Using AIU_TOPO_FILE=/etc/aiu/topo.json
[ 0/ 1]: PT compile complete, took 211.912s
[ 0/ 1]: cpu validation info extracted for validation level 0 and validation level 1 (iter=0)
[ 0/ 1]: aiu validation info extracted for validation level 0
[ 0/ 1]: failed validation level 0, testing validation level 1
[ 0/ 1]: aiu validation info extracted for validation level 1 - iter=0
[ 0/ 1]: cpu validation info extracted for validation level 1 - iter=1
[ 0/ 1]: aiu validation info extracted for validation level 1 - iter=1
[ 0/ 1]: cpu validation info extracted for validation level 1 - iter=2
[...] (iteractions removed for better readability)
[ 0/ 1]: aiu validation info extracted for validation level 1 - iter=60
[ 0/ 1]: cpu validation info extracted for validation level 1 - iter=61
[ 0/ 1]: aiu validation info extracted for validation level 1 - iter=61
[ 0/ 1]: cpu validation info extracted for validation level 1 - iter=62
[ 0/ 1]: aiu validation info extracted for validation level 1 - iter=62
[ 0/ 1]: cpu validation info extracted for validation level 1 - iter=63
[ 0/ 1]: aiu validation info extracted for validation level 1 - iter=63
[ 0/ 1]: mean diff failure rate: 0.7638888888888888
[ 0/ 1]: cross entropy loss failure rate: 0.000992063492063492
===================================================================================== fixture duration top =====================================================================================
total          name               num avg            min           
0:00:00.000130        grand total   5 0:00:00.000012 0:00:00.000009
==================================================================================== test call duration top ====================================================================================
total          name               num avg            min           
0:16:31.480337 test_common_shapes   1 0:16:31.480337 0:16:31.480337
0:16:31.480337        grand total   1 0:16:31.480337 0:16:31.480337
=================================================================================== test setup duration top ====================================================================================
total          name               num avg            min           
0:00:00.000555        grand total   1 0:00:00.000555 0:00:00.000555
================================================================================== test teardown duration top ==================================================================================
total          name               num avg            min           
0:00:00.001416        grand total   1 0:00:00.001416 0:00:00.001416
=================================================================================== short test summary info ====================================================================================
FAILED tests/models/test_decoders.py::test_common_shapes[/mnt/aiu-models-en-shared/models/hf/Mistral-7B-Instruct-v0.3-1-64-16] - AssertionError: failure rate for mean diff was too high: 0.7638888888888888
assert 0.7638888888888888 < 0.01
```

## 4. Run `test_model_expectations.py`

### Test Case for Single Output

First, add the desired model to the `decoder_models` list in [the script](./models/test_model_expectations.py). If the model(s) are too large, it's valid to add a micro model version for this test.

Next, run the following command to save the model weights:

```bash
pytest tests/models/test_model_expectations.py::TestAIUDecoderModels --capture_expectation
```

You should see output similar to this:

```bash
FAILED tests/models/test_model_expectations.py::TestAIUDecoderModels::test_model_output[/tmp/models/mistralai/Mistral-7B-Instruct-v0.3-True] - Failed: Signature file has been saved, please re-run the tests without --capture_expectation
FAILED tests/models/test_model_expectations.py::TestAIUDecoderModels::test_model_weight_keys[/tmp/models/mistralai/Mistral-7B-Instruct-v0.3-True] - Failed: Weights Key file has been saved, please re-run the tests without --capture_expectation
```

This indicates that the expected outputs and weights have been saved successfully, so you'll be able to run the complete suite again to get the test results.

Now, re-run the tests without the `--capture_expectation` flag to validate the model against the saved references and view the actual test results:

```bash
[1000780000@e2e-vllm-dt2-646f66647b-68dh6 aiu-fms-testing-utils]$ pytest tests/models/test_model_expectations.py::TestAIUDecoderModels -vv
[ 0/ 1]: Sentient AIU: Enabled
===================================================================================== test session starts ======================================================================================
platform linux -- Python 3.12.5, pytest-8.3.5, pluggy-1.5.0 -- /usr/bin/python3.12
cachedir: .pytest_cache
rootdir: /tmp/aiu-fms-testing-utils
plugins: durations-1.5.2, env-1.1.5
collected 3 items                                                                                                                                                                              

tests/models/test_model_expectations.py::TestAIUDecoderModels::test_model_output[/tmp/models/mistralai/Mistral-7B-Instruct-v0.3-False] <- ../foundation-model-stack/fms/testing/_internal/model_test_suite.py PASSED [ 33%]
tests/models/test_model_expectations.py::TestAIUDecoderModels::test_model_weight_keys[/tmp/models/mistralai/Mistral-7B-Instruct-v0.3-False] <- ../foundation-model-stack/fms/testing/_internal/model_test_suite.py PASSED [ 66%]
tests/models/test_model_expectations.py::TestAIUDecoderModels::test_model_unfused[/tmp/models/mistralai/Mistral-7B-Instruct-v0.3] SKIPPED (All AIU models are already unfused)           [100%]

===================================================================================== fixture duration top =====================================================================================
total          name                                    num avg            min           
0:00:02.201162                     uninitialized_model   1 0:00:02.201162 0:00:02.201162
0:00:00.051478                                   model   1 0:00:00.051478 0:00:00.051478
0:00:02.252951                             grand total   6 0:00:00.000135 0:00:00.000046
==================================================================================== test call duration top ====================================================================================
total          name                                    num avg            min           
0:03:05.951278 TestAIUDecoderModels::test_model_output   1 0:03:05.951278 0:03:05.951278
0:03:05.954470                             grand total   3 0:00:00.003095 0:00:00.000097
=================================================================================== test setup duration top ====================================================================================
total          name                                    num avg            min           
0:00:00.002004                             grand total   3 0:00:00.000289 0:00:00.000102
================================================================================== test teardown duration top ==================================================================================
total          name                                    num avg            min           
0:00:00.000363                             grand total   3 0:00:00.000090 0:00:00.000077
=========================================================================== 2 passed, 1 skipped in 189.01s (0:03:09) ===========================================================================

```

In the example above, the model tested was a decoder model with a single output, so the `TestAIUDecoderModels` is the most important test case.

### Test Case for Multiple Outputs

If the model produces multiple output tensors, the `TestAIUModelsTupleOutput` test case applies. For example, the default `tuple_output_models` list in [the script](./models/test_model_expectations.py) includes a RoBERTa model that returns outputs in this tuple format.

To test such models, start by adding the desired model to the `tuple_output_models` list in [the script](./models/test_model_expectations.py).

Then, run the following command to save the model weights:

```bash
pytest tests/models/test_model_expectations.py::TestAIUModelsTupleOutput --capture_expectation
```

```bash
tests/models/test_model_expectations.py::TestAIUModelsTupleOutput::test_model_output[/ibm-granite/granite-3.3-8b-instruct/20240603-False] <- ../foundation-model-stack/fms/testing/_internal/model_test_suite.py PASSED [ 66%]
tests/models/test_model_expectations.py::TestAIUModelsTupleOutput::test_model_weight_keys[/ibm-granite/granite-3.3-8b-instruct/20240603-False] <- ../foundation-model-stack/fms/testing/_internal/model_test_suite.py PASSED [ 83%]
tests/models/test_model_expectations.py::TestAIUModelsTupleOutput::test_model_unfused[/ibm-granite/granite-3.3-8b-instruct/20240603] SKIPPED     [100%]
```

### Adding new expectations

When adding new model expectations' tests, please ensure the following are included in the PR:

- expectation tests in the form of pytests
- the data of the image used to generate the file and versions of other key components

Have a look at [this example](https://github.com/foundation-model-stack/aiu-fms-testing-utils/pull/48) of a PR adding new model expectations' files and results.
